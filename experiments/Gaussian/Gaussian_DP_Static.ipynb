{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7hwEWR90zEz"
      },
      "outputs": [],
      "source": [
        "!pip install flexible-fl opacus SciencePlots setuptools flexclash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from flex.data import Dataset, FedDataDistribution, FedDataset, FedDatasetConfig\n",
        "from flex.model import FlexModel\n",
        "from flex.pool import FlexPool, fed_avg\n",
        "from flex.pool.decorators import (\n",
        "    deploy_server_model,\n",
        "    init_server_model,\n",
        "    set_aggregated_weights,\n",
        "    collect_clients_weights,\n",
        ")\n",
        "from flexclash.data import data_poisoner\n",
        "from flexclash.pool.defences import central_differential_privacy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import scienceplots\n",
        "from typing import List\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "from opacus.accountants.utils import get_noise_multiplier\n",
        "from scipy.optimize import linprog\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# --- CONSTANTS ---\n",
        "ROUNDS = 100\n",
        "EPOCHS = 1\n",
        "N_NODES = 10\n",
        "POISONED = 1\n",
        "fixed_epsilon = 1.0\n",
        "fixed_delta = 0.001\n",
        "budget = 100.0\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cifar_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "hNiTowq2072D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset():\n",
        "    \"\"\"\n",
        "    Get the Federated CIFAR10 dataset.\n",
        "    \"\"\"\n",
        "    train_data = CIFAR10(root=\".\", train=True, download=True, transform=None)\n",
        "    test_data = CIFAR10(root=\".\", train=False, download=True, transform=None)\n",
        "    flex_dataset = Dataset.from_torchvision_dataset(train_data)\n",
        "    test_data = Dataset.from_torchvision_dataset(test_data)\n",
        "    assert isinstance(flex_dataset, Dataset)\n",
        "\n",
        "    config = FedDatasetConfig(seed=0)\n",
        "    config.replacement = False\n",
        "    config.n_nodes = N_NODES\n",
        "\n",
        "    flex_dataset = FedDataDistribution.from_config(flex_dataset, config)\n",
        "\n",
        "    assert isinstance(flex_dataset, FedDataset)\n",
        "    flex_dataset[\"server\"] = test_data\n",
        "\n",
        "    return flex_dataset"
      ],
      "metadata": {
        "id": "ZTU9GGULTcfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes = 10):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)\n",
        "\n",
        "def get_model(num_classes=10):\n",
        "  return ModuleValidator.fix(CNNModel(num_classes=num_classes))"
      ],
      "metadata": {
        "id": "xGGuObcyTchU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
        "    \"\"\"\n",
        "    Train the model on the client data.\n",
        "    \"\"\"\n",
        "    model = client_flex_model[\"model\"]\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    torch_dataset = client_data.to_torchvision_dataset(transform=cifar_transforms)\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](model.parameters(), **client_flex_model[\"optimizer_kwargs\"])\n",
        "    dataloader = DataLoader(\n",
        "        torch_dataset, batch_size=32, shuffle=True, pin_memory=False\n",
        "    )\n",
        "\n",
        "    for _ in range(EPOCHS):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    return running_loss\n",
        "\n",
        "def evaluate_model(server_flex_model: FlexModel, data):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the server data.\n",
        "    \"\"\"\n",
        "    data = flex_dataset[\"server\"]\n",
        "    model = server_flex_model[\"model\"]\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    total_count = 0\n",
        "    model = model.to(device)\n",
        "    criterion = server_flex_model[\"criterion\"]\n",
        "\n",
        "    test_dataset = data.to_torchvision_dataset(transform=cifar_transforms)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=32, shuffle=True, pin_memory=False\n",
        "    )\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_dataloader:\n",
        "            total_count += target.size(0)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            losses.append(criterion(output, target).item())\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
        "\n",
        "    test_loss = sum(losses) / len(losses)\n",
        "    test_acc /= total_count\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "ygCklApiTcoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FLEX Decorators\n",
        "@init_server_model\n",
        "def build_server_model():\n",
        "    server_flex_model = FlexModel()\n",
        "    server_flex_model[\"model\"] = get_model()\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
        "    return server_flex_model\n",
        "\n",
        "\n",
        "@deploy_server_model\n",
        "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
        "    new_flex_model = FlexModel()\n",
        "    new_flex_model[\"model\"] = copy.deepcopy(server_flex_model[\"model\"])\n",
        "    new_flex_model[\"server_model\"] = copy.deepcopy(server_flex_model[\"model\"])\n",
        "    new_flex_model[\"discriminator\"] = copy.deepcopy(server_flex_model[\"model\"])\n",
        "    new_flex_model[\"criterion\"] = copy.deepcopy(server_flex_model[\"criterion\"])\n",
        "    new_flex_model[\"optimizer_func\"] = copy.deepcopy(\n",
        "        server_flex_model[\"optimizer_func\"]\n",
        "    )\n",
        "    new_flex_model[\"optimizer_kwargs\"] = copy.deepcopy(\n",
        "        server_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    return new_flex_model\n",
        "\n",
        "@set_aggregated_weights\n",
        "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
        "    dev = aggregated_weights[0].get_device()\n",
        "    dev = \"cpu\" if dev == -1 else \"cuda\"\n",
        "    with torch.no_grad():\n",
        "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
        "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
        "            weight_dict[layer_key].copy_(weight_dict[layer_key].to(dev) + new)\n",
        "\n",
        "@collect_clients_weights\n",
        "def get_clients_weights(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    server_dict = client_flex_model[\"server_model\"].state_dict()\n",
        "    dev = [weight_dict[name] for name in weight_dict][0].get_device()\n",
        "    dev = \"cpu\" if dev == -1 else \"cuda\"\n",
        "    return [\n",
        "        (weight_dict[name] - server_dict[name].to(dev)).type(torch.float)\n",
        "        for name in weight_dict\n",
        "    ]\n",
        "\n",
        "@collect_clients_weights\n",
        "def gaussian_collect(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    dev = [weight_dict[name] for name in weight_dict][0].get_device()\n",
        "    dev = \"cpu\" if dev == -1 else \"cuda\"\n",
        "    return [torch.randn_like(weight_dict[name].float(), device=dev) for name in weight_dict]"
      ],
      "metadata": {
        "id": "FSCUZhbB09u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_attack_optimize_DP(pool: FlexPool):\n",
        "    \"\"\"\n",
        "    Run the attack and optimize the epsilon and delta parameters.\n",
        "    \"\"\"\n",
        "    server = pool.servers\n",
        "    clients = pool.clients\n",
        "    poisoned_clients_ids = list(flex_dataset.keys())[:POISONED]\n",
        "    poisoned_clients = pool.clients.select(\n",
        "        lambda client_id, _: client_id in poisoned_clients_ids\n",
        "    )\n",
        "    clean_clients = pool.clients.select(\n",
        "        lambda client_id, _: client_id not in poisoned_clients_ids\n",
        "    )\n",
        "\n",
        "    epsilon_used = 0\n",
        "    losses = []\n",
        "    accuracies=[]\n",
        "    epsilon_cummulative = []\n",
        "    asr_over_rounds = []\n",
        "\n",
        "    for i in range(ROUNDS):\n",
        "\n",
        "        print(f\"\\n - Round {i+1}: Aggregating with with ε={fixed_epsilon:.3f}, δ={fixed_delta:.5f}\")\n",
        "        server.map(copy_server_model_to_clients, clients)\n",
        "\n",
        "\n",
        "        epsilon_used += fixed_epsilon\n",
        "        loss = clients.map(train)\n",
        "        losses.append(loss[0])\n",
        "\n",
        "        sensitivity = 0.01\n",
        "        noise_multiplier = (sensitivity / fixed_epsilon) * np.sqrt(2 * np.log(1.25 / fixed_delta))\n",
        "        print(\"Noise multiplier\", noise_multiplier)\n",
        "\n",
        "        pool.servers.map(get_clients_weights, clean_clients)\n",
        "        pool.servers.map(gaussian_collect, poisoned_clients)\n",
        "        pool.servers.map(central_differential_privacy, l2_clip = 1.0, noise_multiplier = noise_multiplier)\n",
        "        pool.servers.map(set_agreggated_weights_to_server, pool.servers)\n",
        "\n",
        "        round_metrics = pool.servers.map(evaluate_model)\n",
        "        accuracies.append(round_metrics[0][1]*100)\n",
        "        print(\" * Round metrics: \", round_metrics)\n",
        "        epsilon_cummulative.append(epsilon_used)\n",
        "        print(f\"Epsilon used: {epsilon_used} \\n\")\n",
        "\n",
        "        with open(\"experiment_DP_Static.txt\", \"a\") as archivo:\n",
        "            archivo.write(f\"\\n - Round {i+1}: Training with ε={fixed_epsilon:.3f}, δ={fixed_delta:.5f}\\n\")\n",
        "            archivo.write(f\"Round metrics: {round_metrics}\\n\")\n",
        "            archivo.write(f\"Epsilon used: {epsilon_used}\\n\")\n",
        "            archivo.write(\"-\" * 30 + \"\\n\")\n",
        "\n",
        "    df_metrics = pd.DataFrame({\n",
        "      'Round': list(range(1, ROUNDS + 1)),\n",
        "      'Accuracy (%)': accuracies,\n",
        "      'Epsilon Acumulado': epsilon_cummulative,\n",
        "      })\n",
        "    df_metrics.to_csv('metrics_DP_Static.csv', index=False)"
      ],
      "metadata": {
        "id": "2KIbqxxO09yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    flex_dataset = get_dataset()\n",
        "    pool = FlexPool.client_server_pool(\n",
        "        fed_dataset=flex_dataset, init_func=build_server_model\n",
        "    )\n",
        "    run_attack_optimize_DP(pool)"
      ],
      "metadata": {
        "id": "ekyWWxGs090r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}