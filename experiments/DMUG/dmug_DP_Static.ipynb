{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3EG-u2yneTqG"
      },
      "outputs": [],
      "source": [
        "!pip install flexible-fl opacus SciencePlots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWhuwXazeZG_"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from flex.data import Dataset, FedDataDistribution, FedDataset, FedDatasetConfig\n",
        "from flex.model import FlexModel\n",
        "from flex.pool import FlexPool, fed_avg\n",
        "from flex.pool.decorators import (\n",
        "    deploy_server_model,\n",
        "    init_server_model,\n",
        "    set_aggregated_weights,\n",
        "    collect_clients_weights,\n",
        ")\n",
        "from typing import List\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "from scipy.optimize import linprog\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "\n",
        "# --- CONSTANTS ---\n",
        "ROUNDS = 100\n",
        "EPOCHS = 1\n",
        "N_CLIENTS = 2\n",
        "fixed_epsilon = 1.0\n",
        "fixed_delta = 0.001\n",
        "budget = 100.0\n",
        "GENERATOR_EPOCHS = 100\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCAxEqhcGe22"
      },
      "outputs": [],
      "source": [
        "def get_dataset():\n",
        "    \"\"\"\n",
        "    Creation of the Federated MNIST datset\n",
        "    \"\"\"\n",
        "    train_data = MNIST(root=\".\", train=True, download=True, transform=None)\n",
        "    test_data = MNIST(root=\".\", train=False, download=True, transform=None)\n",
        "    flex_dataset = Dataset.from_torchvision_dataset(train_data)\n",
        "    test_data = Dataset.from_torchvision_dataset(test_data)\n",
        "    assert isinstance(flex_dataset, Dataset)\n",
        "\n",
        "    config = FedDatasetConfig(seed=0)\n",
        "    config.replacement = False\n",
        "    config.n_nodes = N_CLIENTS\n",
        "    config.labels_per_node = [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n",
        "\n",
        "    flex_dataset = FedDataDistribution.from_config(flex_dataset, config)\n",
        "\n",
        "    assert isinstance(flex_dataset, FedDataset)\n",
        "    flex_dataset[\"server\"] = test_data\n",
        "\n",
        "    return flex_dataset\n",
        "\n",
        "# Create the dataset and transform object\n",
        "flex_dataset = get_dataset()\n",
        "\n",
        "mnist_transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Lambda(lambda x: 2 * x - 1)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTMX0sdZKLv2"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(1024, 200)\n",
        "        self.fc2 = nn.Linear(200, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.tanh(self.conv1(x)))\n",
        "        x = self.pool(torch.tanh(self.conv2(x)))\n",
        "        x = x.view(-1, 1024)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def get_model(num_classes=11):\n",
        "  \"\"\"\n",
        "  Creation of the model, will be passed to build_server_model(). Num classes must be 10 (digits) + 1 (fake image)\n",
        "  \"\"\"\n",
        "  return ModuleValidator.fix(CNNModel(num_classes=num_classes))\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim=100, output_dim=1, img_size=28):\n",
        "        super(Generator, self).__init__()\n",
        "        self.init_size = img_size // 4\n",
        "        self.l1 = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size**2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, output_dim, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy5HFCtGG1He"
      },
      "outputs": [],
      "source": [
        "# FLEX decorators\n",
        "@init_server_model\n",
        "def build_server_model():\n",
        "    server_flex_model = FlexModel()\n",
        "    server_flex_model[\"model\"] = get_model()\n",
        "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.SGD\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {\n",
        "        \"lr\": 1e-3,\n",
        "        \"weight_decay\": 1e-7,\n",
        "        \"momentum\": 0,\n",
        "    }\n",
        "    return server_flex_model\n",
        "\n",
        "\n",
        "@deploy_server_model\n",
        "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
        "    new_flex_model = FlexModel()\n",
        "    new_flex_model[\"model\"] = copy.deepcopy(server_flex_model[\"model\"])\n",
        "    new_flex_model[\"server_model\"] = copy.deepcopy(server_flex_model[\"model\"])\n",
        "    new_flex_model[\"discriminator\"] = copy.deepcopy(server_flex_model[\"model\"])\n",
        "    new_flex_model[\"criterion\"] = copy.deepcopy(server_flex_model[\"criterion\"])\n",
        "    new_flex_model[\"optimizer_func\"] = copy.deepcopy(\n",
        "        server_flex_model[\"optimizer_func\"]\n",
        "    )\n",
        "    new_flex_model[\"optimizer_kwargs\"] = copy.deepcopy(\n",
        "        server_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    return new_flex_model\n",
        "\n",
        "@set_aggregated_weights\n",
        "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
        "    dev = aggregated_weights[0].get_device()\n",
        "    dev = \"cpu\" if dev == -1 else \"cuda\"\n",
        "    with torch.no_grad():\n",
        "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
        "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
        "            weight_dict[layer_key].copy_(weight_dict[layer_key].to(dev) + new)\n",
        "\n",
        "\n",
        "@collect_clients_weights\n",
        "def get_clients_weights(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    server_dict = client_flex_model[\"server_model\"].state_dict()\n",
        "    dev = [weight_dict[name] for name in weight_dict][0].get_device()\n",
        "    dev = \"cpu\" if dev == -1 else \"cuda\"\n",
        "    return [\n",
        "        (weight_dict[name] - server_dict[name].to(dev)).type(torch.float)\n",
        "        for name in weight_dict\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-br_DwxFG1Jj"
      },
      "outputs": [],
      "source": [
        "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
        "    \"\"\"\n",
        "    Train the model on the client data.\n",
        "    \"\"\"\n",
        "    model = client_flex_model[\"model\"]\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    torch_dataset = client_data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    print(f\"Client data: {len(torch_dataset)}\")\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    dataloader = DataLoader(\n",
        "        torch_dataset, batch_size=64, shuffle=True, pin_memory=False\n",
        "    )\n",
        "\n",
        "    for _ in range(EPOCHS):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    return running_loss\n",
        "\n",
        "\n",
        "def dp_train(client_flex_model: FlexModel, client_data: Dataset, epsilon_t: float, delta_t: float):\n",
        "    \"\"\"\n",
        "    Train the model on the client data using differential privacy. To do so, Opacus offers a make_private_with_epsilon\n",
        "    function to create a DP private model.\n",
        "\n",
        "    :param epsilon_t: Target epsilon value\n",
        "    :param delta_t: Target delta value\n",
        "    \"\"\"\n",
        "    model = client_flex_model[\"model\"]\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "    privacy_engine = PrivacyEngine()\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    torch_dataset = client_data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    dataloader = DataLoader(\n",
        "        torch_dataset, batch_size=64, shuffle=True, pin_memory=False\n",
        "    )\n",
        "    model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(\n",
        "        module=model,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=dataloader,\n",
        "        target_epsilon=epsilon_t,\n",
        "        target_delta=delta_t,\n",
        "        epochs=EPOCHS,\n",
        "        max_grad_norm=1.0,\n",
        "        grad_sample_mode=\"hooks\",\n",
        "    )\n",
        "    for _ in range(EPOCHS):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    return running_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60WSOAHcG1QX"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(server_flex_model: FlexModel, data):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the server data.\n",
        "    \"\"\"\n",
        "    data = flex_dataset[\"server\"]\n",
        "    model = server_flex_model[\"model\"]\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    total_count = 0\n",
        "    model = model.to(device)\n",
        "    criterion = server_flex_model[\"criterion\"]\n",
        "\n",
        "    test_dataset = data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=256, shuffle=True, pin_memory=False\n",
        "    )\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_dataloader:\n",
        "            total_count += target.size(0)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            losses.append(criterion(output, target).item())\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
        "\n",
        "    test_loss = sum(losses) / len(losses)\n",
        "    test_acc /= total_count\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4b8xDTPHXkX"
      },
      "outputs": [],
      "source": [
        "def train_benign(pool: FlexPool, epsilon_t: float, delta_t: float):\n",
        "    \"\"\"\n",
        "    Train the model on the server data.\n",
        "    \"\"\"\n",
        "    benign_client = pool.select(lambda id, role: id == 0)\n",
        "    pool.servers.map(copy_server_model_to_clients, benign_client)\n",
        "    if epsilon_t == 0:\n",
        "      losses = benign_client.map(train)\n",
        "    else:\n",
        "      losses = benign_client.map(dp_train, epsilon_t=epsilon_t, delta_t=delta_t)\n",
        "    return losses\n",
        "\n",
        "# Labels must be the same type\n",
        "class TensorLabelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, wrapped_dataset):\n",
        "        self.wrapped_dataset = wrapped_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wrapped_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data, label = self.wrapped_dataset[idx]\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return data, label\n",
        "\n",
        "\n",
        "def merge_dataset(client_data: Dataset, fake_images: torch.Tensor, fake_labels: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Merge the client data with the fake images.\n",
        "    \"\"\"\n",
        "    dataset = TensorLabelDataset(\n",
        "        client_data.to_torchvision_dataset(transform=mnist_transforms)\n",
        "    )\n",
        "    fake_images = fake_images.detach().cpu()\n",
        "    fake_dataset = torch.utils.data.TensorDataset(fake_images, fake_labels)\n",
        "    train_dataset = torch.utils.data.ConcatDataset([dataset, fake_dataset])\n",
        "    return train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-2jBBTtHXm0"
      },
      "outputs": [],
      "source": [
        "def optimize_gan(flex_model: FlexModel, client_data: Dataset, label: int = 0):\n",
        "    \"\"\"\n",
        "    Train the GAN on the client data.\n",
        "    \"\"\"\n",
        "    if \"generator\" not in flex_model:\n",
        "        flex_model[\"generator\"] = Generator()\n",
        "    discriminator = flex_model[\"discriminator\"].to(device)\n",
        "    generator = flex_model[\"generator\"].to(device)\n",
        "    criterion = flex_model[\"criterion\"]\n",
        "\n",
        "    generator_optimizer = torch.optim.SGD(\n",
        "        generator.parameters(), lr=0.02, weight_decay=1e-5\n",
        "    )\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    generator_loss = torch.tensor(float(\"inf\"))\n",
        "\n",
        "    for _ in range(GENERATOR_EPOCHS):\n",
        "        generator_optimizer.zero_grad()\n",
        "        noise = torch.randn(128, 100, device=device)\n",
        "        fake_images = generator(noise)\n",
        "        outputs = discriminator(fake_images)\n",
        "        fake_labels = torch.full((128,), label, dtype=torch.long, device=device)\n",
        "        generator_loss = criterion(outputs, fake_labels)\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "    return generator_loss\n",
        "\n",
        "\n",
        "def train_with_fake_images(client_flex_model: FlexModel, client_data: Dataset):\n",
        "    \"\"\"\n",
        "    Train the model on the client data with the fake images.\n",
        "    \"\"\"\n",
        "    size = len(client_data) // 4\n",
        "    fake_images = client_flex_model[\"generator\"].to(device)(\n",
        "        torch.randn(size, 100, device=device)\n",
        "    )\n",
        "    fake_labels = torch.full((size,), 10, dtype=torch.long, device=\"cpu\")\n",
        "    train_dataset = merge_dataset(client_data, fake_images, fake_labels)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        train_dataset, batch_size=64, shuffle=True, pin_memory=False\n",
        "    )\n",
        "    model = client_flex_model[\"model\"]\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    for _ in range(EPOCHS):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    return running_loss\n",
        "\n",
        "\n",
        "def train_malicious(pool: FlexPool):\n",
        "    \"\"\"\n",
        "    Train the model on the server data.\n",
        "    \"\"\"\n",
        "    malicious_client = pool.select(lambda id, role: id == 1)\n",
        "    pool.servers.map(copy_server_model_to_clients, malicious_client)\n",
        "    gen_loss = malicious_client.map(optimize_gan, label=0)[0]\n",
        "    loss = malicious_client.map(train_with_fake_images)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def extract_fake_image(pool: FlexPool, i: int):\n",
        "    \"\"\"\n",
        "    Extract the fake image from the server.\n",
        "    \"\"\"\n",
        "    malicious_client = pool.select(lambda id, role: id == 1)\n",
        "    noise = torch.randn(1, 100, device=device)\n",
        "    fake_image = malicious_client.map(\n",
        "        lambda flex_model, _: flex_model[\"generator\"].to(device)(noise)\n",
        "    )[0]\n",
        "    fake_image = fake_image[0].detach().cpu().numpy()\n",
        "    plt.imsave(f\"images/fake_image_{i}.png\", fake_image.squeeze(), cmap=\"gray\")\n",
        "    return get_fake_image_min_distance(fake_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtu4R7X9HXpG"
      },
      "outputs": [],
      "source": [
        "def get_fake_image_min_distance(fake_image):\n",
        "    \"\"\"\n",
        "    Get the minimum distance between the fake image and the training data. Will serve as an attack success metric.\n",
        "    \"\"\"\n",
        "    global flex_dataset\n",
        "    train_dataset = flex_dataset[\"server\"].to_torchvision_dataset(transform=mnist_transforms)\n",
        "    min_distance = float('inf')\n",
        "    for image, label in train_dataset:\n",
        "      if label == 0:\n",
        "        image = image.numpy()\n",
        "        distance = np.linalg.norm(fake_image - image)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "    return min_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xmJ75GRH-pX"
      },
      "outputs": [],
      "source": [
        "def run_attack_optimize_DP(pool: FlexPool):\n",
        "    \"\"\"\n",
        "    Run the attack and optimize the epsilon and delta parameters. It saves the metrics in a csv file.\n",
        "    \"\"\"\n",
        "    malicious_client = pool.select(lambda id, role: id == 1)\n",
        "    benign_client = pool.select(lambda id, role: id == 0)\n",
        "    epsilon_used = 0\n",
        "    losses = []\n",
        "    accuracies=[]\n",
        "    epsilon_cummulative = []\n",
        "    min_distances = []\n",
        "\n",
        "    for i in range(ROUNDS):\n",
        "\n",
        "        print(f\"\\n - Round {i+1}: Training with ε={fixed_epsilon:.3f}, δ={fixed_delta:.5f}\")\n",
        "        loss_t = train_benign(pool, fixed_epsilon, fixed_delta)\n",
        "        epsilon_used += fixed_epsilon\n",
        "        losses.append(loss_t[0])\n",
        "\n",
        "        pool.servers.map(get_clients_weights, benign_client)\n",
        "        pool.servers.map(fed_avg)\n",
        "        pool.servers.map(set_agreggated_weights_to_server, pool.servers)\n",
        "        round_metrics = pool.servers.map(evaluate_model)\n",
        "        accuracies.append(round_metrics[0][1]*100)\n",
        "        print(\" * Round metrics: \", round_metrics)\n",
        "        malicious_loss = train_malicious(pool)\n",
        "        pool.servers.map(get_clients_weights, malicious_client)\n",
        "        pool.servers.map(fed_avg)\n",
        "        pool.servers.map(set_agreggated_weights_to_server, pool.servers)\n",
        "        min_dist = extract_fake_image(pool, i)\n",
        "        print(\"Fake image min distance \", min_dist)\n",
        "        min_distances.append(min_dist)\n",
        "        epsilon_cummulative.append(epsilon_used)\n",
        "        print(f\"Epsilon used: {epsilon_used} \\n\")\n",
        "\n",
        "        with open(\"experimento_DP_Static.txt\", \"a\") as archivo:\n",
        "            archivo.write(f\"\\n - Round {i+1}: Training with ε={fixed_epsilon:.3f}, δ={fixed_delta:.5f}\\n\")\n",
        "            archivo.write(f\"Round metrics: {round_metrics}\\n\")\n",
        "            archivo.write(f\"Epsilon used: {epsilon_used}\\n\")\n",
        "            archivo.write(\"-\" * 30 + \"\\n\")\n",
        "\n",
        "    df_metrics = pd.DataFrame({\n",
        "    'Round': list(range(1, ROUNDS + 1)),\n",
        "    'Accuracy (%)': accuracies,\n",
        "    'Epsilon Acumulado': epsilon_cummulative,\n",
        "    'Distancia Minima': min_distances\n",
        "    })\n",
        "    df_metrics.to_csv('metricas_DP_Static.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV-pgJyKH-rz"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    pool = FlexPool.client_server_pool(\n",
        "        fed_dataset=flex_dataset, init_func=build_server_model\n",
        "    )\n",
        "    run_attack_optimize_DP(pool)\n",
        "    im_attack_recovered = plt.imread(f'images/fake_image_{ROUNDS-1}.png')\n",
        "    plt.imshow(im_attack_recovered, cmap=\"gray\")\n",
        "    plt.savefig('recovered_static.png', dpi=300)\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
